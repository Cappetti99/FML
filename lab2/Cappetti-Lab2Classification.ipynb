{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_P63ycwnX0H0"
   },
   "source": [
    "# Classification in Practice\n",
    "\n",
    "In this laboratory session we will gain some experience working with linear models for **classification** (we already saw how to do regression in a previous lab).\n",
    "\n",
    "We will follow the same general structure... with one exception -- for this laboratory we will begin with **synthetic** datasets. \n",
    "\n",
    "## Part 1: Working with Synthetic Classification Problems\n",
    "\n",
    "Generating synthetic datasets allows us to gain insight into how classifiers work. We will use some functionality of Scikit-learn to generate -- in a controlled manner -- synthetic classification problems of with various characteristics.\n",
    "\n",
    "**Note**: When working with these synthetic datasets we will *not* go to the trouble of generating train/test splits -- we are only interested in studying how classifiers work to **separate** the training data.\n",
    "\n",
    "### Exercise 1.1: Generating a dataset\n",
    "\n",
    "First, have a look at the documentation for [sklearn.datasets.make_blobs](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html#sklearn.datasets.make_blobs). This is one of the easiest ways to generate /simple/ classification problems. Study the documentation and then use `make_blobs` to generate an **EASY** dataset for a **two-class** classification problem with 100 samples per class and 2 input features. What does **EASY** mean? How can we determine, qualitatively, that the randomly generated dataset is \"easy\"? Make the problem *easy*, but not *too easy*.\n",
    "\n",
    "**Hint**: You will probably want to develop a *visualization* for datasets, and then *abstract* it into a function you can call later for other datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn.covariance._empirical_covariance\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Variables are collinear\", module=\"sklearn.discriminant_analysis\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Variables are collinear\", module=\"sklearn.metrics\")\n",
    "\n",
    "#set a random seed\n",
    "#np.random.seed(100)\n",
    "\n",
    "def visualizer(X, y):\n",
    "    plt.figure()\n",
    "    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red')\n",
    "    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue')\n",
    "    plt.legend(['Class 0', 'Class 1'])\n",
    "    plt.show()\n",
    "\n",
    "x, y = make_blobs(n_samples=100, centers= 2, cluster_std=0.5)\n",
    "visualizer(x, y)\n",
    "x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image generated by my code shows two distinct clusters, one in red and the other in blue. These clusters represent samples generated using make_blobs with centers=2, which creates two groups of data points.\n",
    "\n",
    "Since i used cluster_std=0.5, each cluster has a moderate spread, which is reflected in the fact that the points are loosely but distinctly grouped in each color.\n",
    "\n",
    "I use this link to study and create blobs: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html#sklearn.datasets.make_blobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2: \"Solving\" the dataset\n",
    "\n",
    "Train a classifier that is *perfectly* classifies the dataset you created above. Any of the the three classifiers mentioned in the *Capsule Lecture* should do well. Try one, or try all three. Be sure to verify that the classifier does *indeed* classify all training points correctly.\n",
    "\n",
    "**Hint**: You might want to look at [sklearn.metrics.classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import LinearSVC #support vector classifier\n",
    "from  sklearn.metrics import classification_report\n",
    "\n",
    "def visualizeClassifier(lda, x, y):\n",
    "    lda.fit(x, y)  # Fit the LinearDiscriminantAnalysis model to the data\n",
    "    correct = (lda.predict(x) == y)  # Check if predictions match ground truth\n",
    "    visualizer(x, y)\n",
    "    accuracy = sum(correct) / len(correct)  # Calculate accuracy\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    # Print a classification report for precision, recall, and F1-score\n",
    "    print(classification_report(y, lda.predict(x)))\n",
    "    \n",
    "# Create a LinearDiscriminantAnalysis classifier\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Call the visualization function to analyze and display the classifier\n",
    "visualizeClassifier(lda, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Linear Support Vector Classifier (SVC) with 'dual' set to True to address a warning\n",
    "svc = LinearSVC(dual=True) \n",
    "\n",
    "visualizeClassifier(svc, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Quadratic Discriminant Analysis (QDA) classifier\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "# Call the visualization function to analyze and display the classifier\n",
    "visualizeClassifier(qda, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows two clearly separated clusters of points in different regions of the graph, represented by red and blue colors, suggesting that the data is highly separable.\n",
    "Because the classes are linearly separable every classifier performs well. I try `LDA` , `SVC` , `QDA`.\n",
    "The result of each classifier give us an accuracy of 1.00."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3: Visualizing the decision surfaces\n",
    "\n",
    "One of the best ways to understand how a classifier works is to visualize the decision boundaries. Use [sklearn.inspection.DecisionBoundaryDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.DecisionBoundaryDisplay.html) to create a visualization of the *dataset* and the *decision boundaries* for your classifier.\n",
    "\n",
    "**Note**: This is another great opportunity to apply *functional abstraction* and make a **reusable** visualization that you can reuse (for example in the next exercise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary function for decision boundary visualization\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# Define a function to visualize the decision boundary of a classifier\n",
    "def visualizeDecisionBoundary(classifier, X, y, cmap=\"bwr\", alpha=0.4, response_method=\"predict\"):\n",
    "    # Fit the classifier on the input data and labels\n",
    "    classifier.fit(X, y)\n",
    "\n",
    "    # Create a decision boundary display using the provided classifier, data, response method, and alpha transparency\n",
    "    disp = DecisionBoundaryDisplay.from_estimator(\n",
    "        classifier, X, response_method=response_method, alpha=alpha\n",
    "    )\n",
    "\n",
    "    # Scatter plot of the input data points with colors based on class labels\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap)\n",
    "\n",
    "# Create an instance of Linear Discriminant Analysis (LDA)\n",
    "ldc = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Visualize the decision boundary of the LDA classifier on the provided dataset\n",
    "visualizeDecisionBoundary(ldc, x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdc = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "visualizeDecisionBoundary(qdc, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of Linear Support Vector Classifier (SVC) with specific parameters\n",
    "svc = LinearSVC(dual=True)\n",
    "\n",
    "# Visualize the decision boundary of the Linear SVC classifier on the dataset\n",
    "visualizeDecisionBoundary(svc, x, y, cmap=\"jet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results return a marked division of the two classes, this is due in part to the clear division between the two.\n",
    "\n",
    "Comparison of LDA, SVC, QDA returns the same division, with a different graph scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.4: A harder dataset\n",
    "\n",
    "Repeat the exercises above, but first generate a **hard** dataset that is not linearly separable. Observe how linear classifiers fail to correctly classify the training data. How can we make these classifiers capable of \"solving\" this harder dataset? Try to find an explicit embedding that makes the problem linearly separable in the embedding space. Visualize the decision boundaries in the **original** space (you will need to spend some time with the documentation for `DecisionBoundaryDisplay` to make this work)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# Generate a non-linear dataset using 'make_moons' with 200 samples and some noise\n",
    "X, y = make_moons(n_samples=200, noise=0.2)\n",
    "\n",
    "# Initialize and train a Support Vector Classifier with a linear kernel\n",
    "linear_svc = SVC(kernel='linear').fit(X, y)\n",
    "\n",
    "linear_qda = QuadraticDiscriminantAnalysis().fit(X, y)  \n",
    "\n",
    "linear_lda = LinearDiscriminantAnalysis().fit(X, y)\n",
    "\n",
    "# Create a pipeline that adds polynomial features (degree 2) and applies SVC with an RBF kernel\n",
    "poly_clf = Pipeline([\n",
    "    (\"poly_features\", PolynomialFeatures(degree=2)),  # Step to add polynomial features\n",
    "    (\"svm_clf\", SVC(kernel='rbf', gamma=0.5))         # SVM with RBF kernel and gamma parameter for non-linearity\n",
    "])\n",
    "# Train the polynomial SVM classifier on the data\n",
    "poly_clf.fit(X, y)\n",
    "\n",
    "# Set up the figure with a specified size for plotting\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot the decision boundary of the linear SVC on the dataset\n",
    "DecisionBoundaryDisplay.from_estimator(linear_svc, X, response_method=\"predict\", cmap=plt.cm.Paired, alpha=0.5)\n",
    "# Scatter plot of the data points, colored by class labels\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor=\"k\", cmap=plt.cm.Paired)\n",
    "plt.title(\"Linear SVC on Non-Linearly Separable Data\")\n",
    "\n",
    "# Set up the second plot for the polynomial SVM classifier\n",
    "DecisionBoundaryDisplay.from_estimator(linear_lda, X, response_method=\"predict\", cmap=plt.cm.Paired, alpha=0.5) \n",
    "# Scatter plot of the same data points for the second plot\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor=\"k\", cmap=plt.cm.Paired)\n",
    "plt.title(\"Linear Discriminant Analysis on Non-Linearly Separable Data\")\n",
    "\n",
    "# Set up the third plot for the polynomial SVM classifier\n",
    "DecisionBoundaryDisplay.from_estimator(linear_qda, X, response_method=\"predict\", cmap=plt.cm.Paired, alpha=0.5)\n",
    "# Scatter plot of the same data points for the second plot\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor=\"k\", cmap=plt.cm.Paired)\n",
    "plt.title(\"Quadratic Discriminant Analysis on Non-Linearly Separable Data\")\n",
    "\n",
    "# Plot the decision boundary of the polynomial SVM classifier with RBF kernel\n",
    "DecisionBoundaryDisplay.from_estimator(poly_clf, X, response_method=\"decision_function\", cmap=plt.cm.Paired, alpha=0.5)\n",
    "# Scatter plot of the same data points for the second plot\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor=\"k\", cmap=plt.cm.Paired)\n",
    "plt.title(\"Polynomial Features with SVC and RBF Kernel\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Accuracy of Linear Discriminant Analysis: \", linear_lda.score(X, y))\n",
    "print(\"Accuracy of Quadratic Discriminant Analysis: \", linear_qda.score(X, y))\n",
    "print(\"Accuracy of Linear SVC: \", linear_svc.score(X, y))\n",
    "print(\"Accuracy of Polynomial SVC: \", poly_clf.score(X, y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Linear Classifier`: The separating line is a straight line (because the kernel is linear), we also observe that many points were misclassified, as the straight line fails to adequately separate the two datasets that have a crescent-shaped distribution. This shows that a linear kernel cannot capture the inherent complexity of the dataset.\n",
    "\n",
    "`Polynomial Features with SVC and RBF Kernel`: Here an SVC classifier with RBF kernels is used, preceded by a step that adds polynomial features of degree 2. The decision boundaries are curved and complex, fitting the distribution of the dataset much better.\n",
    "Regions with very intricate boundaries can be seen, indicating that the model can handle nonlinear shapes effectively, and the RBF kernels also accurately identified the two classes even though they are not linearly separable.\n",
    "\n",
    "I also printed the accuracy, revealing , as we said before, that a Polynomial classifier is very more accurate than the other linear classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section, I got help from Chatgpt Version 4 (the most recent and free), particularly for some parts where I then adjusted the code provided by it. It did not always provide exactly what I wanted but with my adjustments it was effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: A Real Dataset\n",
    "\n",
    "In the second set of exercises we will work with a classic dataset for classification: the Iris Flower Classification Dataset. It is a fairly easy dataset to work with since it is low-dimensional and small. We start by loading the dataset, and then proceed with our usual protocol: \"playing\" with the data, creating train/test splits, and building and evaluating a first classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RvUVJmT0X0H2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the iris classification dataset to get started.\n",
    "ds = load_iris()\n",
    "\n",
    "# Extract the features (Xs), targets (ys), and class names (labels).\n",
    "Xs = ds.data\n",
    "ys = ds.target\n",
    "classes = ds.target_names\n",
    "\n",
    "# Make a Pandas DataFrame too, just for shits and giggles.\n",
    "df = pd.DataFrame(Xs, columns=ds.feature_names)\n",
    "targets = pd.Series(ds.target)\n",
    "\n",
    "# Print the class labels (targets) to check the dataset's structure.\n",
    "print(ds['DESCR'])\n",
    "\n",
    "column_names = ds['feature_names']\n",
    "print(column_names)\n",
    "# Split the dataset into training and testing sets using train_test_split, where Xtr and ytr are training data.\n",
    "Xtr, Xts, ytr, yts = train_test_split(Xs, ys, random_state=42)\n",
    "\n",
    "def new_visualizer(X, y):\n",
    "    plt.figure()\n",
    "    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red')\n",
    "    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue')\n",
    "    plt.scatter(X[y == 2][:, 0], X[y == 2][:, 1], color='green')\n",
    "    plt.show()\n",
    "\n",
    "new_visualizer(Xtr, ytr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. _iris_dataset:\n",
    "\n",
    "Iris plants dataset\n",
    "--------------------\n",
    "\n",
    "**Data Set Characteristics:**\n",
    "\n",
    "    :Number of Instances: 150 (50 in each of three classes)\n",
    "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
    "    :Attribute Information:\n",
    "        - sepal length in cm\n",
    "        - sepal width in cm\n",
    "        - petal length in cm\n",
    "        - petal width in cm\n",
    "        - class:\n",
    "                - Iris-Setosa\n",
    "                - Iris-Versicolour\n",
    "                - Iris-Virginica\n",
    "                \n",
    "    :Summary Statistics:\n",
    "\n",
    "    ============== ==== ==== ======= ===== ====================\n",
    "                    Min  Max   Mean    SD   Class Correlation\n",
    "    ============== ==== ==== ======= ===== ====================\n",
    "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
    "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
    "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
    "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
    "    ============== ==== ==== ======= ===== ====================\n",
    "\n",
    "    :Missing Attribute Values: None\n",
    "    :Class Distribution: 33.3% for each of 3 classes.\n",
    "    :Creator: R.A. Fisher\n",
    "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
    "    :Date: July, 1988\n",
    "\n",
    "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
    "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
    "Machine Learning Repository, which has two wrong data points.\n",
    "\n",
    "This is perhaps the best known database to be found in the\n",
    "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
    "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
    "data set contains 3 classes of 50 instances each, where each class refers to a\n",
    "type of iris plant.  One class is linearly separable from the other 2; the\n",
    "latter are NOT linearly separable from each other.\n",
    "\n",
    ".. topic:: References\n",
    "\n",
    "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
    "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
    "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
    "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
    "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
    "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
    "     Structure and Classification Rule for Recognition in Partially Exposed\n",
    "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
    "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
    "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
    "     on Information Theory, May 1972, 431-433.\n",
    "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
    "     conceptual clustering system finds 3 classes in the data.\n",
    "   - Many, many more ...\n",
    "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Play with the data\n",
    "\n",
    "Use everything we have learned about *exploratory data analysis* to study the nature and characteristics of this classification problem. Are the classes equally represented? How many features are there in input? How are input features scaled? **Be creative** and **summarize** your findings with analysis along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your data playground here.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "\n",
    "# Ignore warnings related to data conversion\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "\n",
    "# 1. Column names (features)\n",
    "print(\"Column names (features):\")\n",
    "print(df.columns)\n",
    "\n",
    "# 2. Calculate the correlation matrix between features\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "# 3. Create a heatmap to visualize correlations with annotations\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Feature correlation matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Sepal Length` (cm):\n",
    "Has a moderate positive correlation with petal length (cm) (0.87) and petal width (cm) (0.82), this indicates that when sepal length increases, petal length and width also tend to increase.\n",
    "It has a very low and negative correlation with sepal width (cm) (-0.12), suggesting that there is no significant relationship between sepal length and width.\n",
    "\n",
    "`Sepal Width` (cm):\n",
    "It is negatively correlated with petal length ( cm) (-0.43) and petal width (cm) (-0.37), meaning that greater sepal width tends to be associated with shorter and narrower petals.\n",
    "It does not show a significant relationship with sepal length ( cm) (-0.12).\n",
    "\n",
    "`Petal Length` (cm):\n",
    "Has a very strong and positive correlation with petal width ( cm) (0.96), this indicates that as petal length increases, petal width also increases.\n",
    "It has a moderate correlation with sepal length (cm) (0.87) and a negative correlation with sepal width (cm) (-0.43).\n",
    "\n",
    "`Petal Width` (cm):\n",
    "Is strongly correlated with petal length (cm) (0.96), showing that these two characteristics are closely related, is also moderately correlated with sepal length (cm) (0.82) and has a negative correlation with sepal width (cm) (-0.37)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FXCheZBX0H7"
   },
   "source": [
    "### Exercise 2.2: Your Turn\n",
    "\n",
    "Design an experiment to decide which of the three classifiers we have seen performs best on this dataset. Some things to keep in mind:\n",
    "+ You will probably want to use [sklearn.model_selection.cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) to perform K-fold cross-validation to ensure you are *robustly* estimating performance.\n",
    "+ All three of the classifier models we have used support **regularization**, which might be an interesting hyperparameter to cross-validate. Unfortunately they use slightly different terminologies:\n",
    "  + in `sklearn.discriminant_analysis.LinearDiscriminantAnalysis` it is called `shrinkage`\n",
    "  + in `sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis` it is called `reg_param`\n",
    "  + and in `sklearn.svm.LinearSVC` it is called `C` -- but the regularization performed is 1/C!\n",
    "  \n",
    "**Important**: Remember to *document* your findings and analyses along the way. Summarize and justify your final conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Function to find the best configuration for Linear Discriminant Analysis (LDA)\n",
    "def bestLDA(Xs, ys):\n",
    "    # List of shrinkage values to evaluate\n",
    "    shrinkage = [None, 'auto', 0.0000001, 0.2, 0.3, 0.4, 0.9]\n",
    "\n",
    "    # Dictionary to store cross-validation results\n",
    "    cross_val_results_LDA = {}\n",
    "\n",
    "    best_shrinkage = None\n",
    "    best_score = 0.0\n",
    "    best_degree_LDA = 0\n",
    "    print(\"\\nBest LDA Configuration:\")\n",
    "    \n",
    "    # Iterate over different polynomial degrees (1 and 2)\n",
    "    for i in range(1, 3):\n",
    "        print(f\"Evaluating Degree {i}:\")\n",
    "        \n",
    "        # Iterate over different shrinkage values\n",
    "        for shrinkage_value in shrinkage:\n",
    "            # Use a pipeline with scaling, polynomial expansion, and LDA\n",
    "            pipeline = make_pipeline(\n",
    "                StandardScaler(),\n",
    "                PolynomialFeatures(degree=i),\n",
    "                LinearDiscriminantAnalysis(solver='lsqr', shrinkage=shrinkage_value, tol=1e-9)\n",
    "            )\n",
    "            \n",
    "            # Perform cross-validation and calculate the mean score\n",
    "            scores = cross_val_score(pipeline, Xs, ys, cv=5, scoring='accuracy')\n",
    "            mean_score = scores.mean()\n",
    "\n",
    "            # Check if the current combination has a higher score\n",
    "            if mean_score > best_score:\n",
    "                best_shrinkage = shrinkage_value\n",
    "                best_score = mean_score\n",
    "                best_degree_LDA = i\n",
    "                \n",
    "            # Print the evaluation results for the current combination\n",
    "            print(f\"  Degree: {i}, Shrinkage: {shrinkage_value}, Score: {mean_score:.4f}\")\n",
    "\n",
    "    # Print the best LDA configuration and its details\n",
    "    print(\"\\nBest LDA Configuration:\")\n",
    "    print(f\"Best Degree: {best_degree_LDA}\")\n",
    "    print(f\"Best Shrinkage: {best_shrinkage}\")\n",
    "    print(f\"Best Score: {best_score:.4f}\")\n",
    "    \n",
    "    return best_degree_LDA, best_shrinkage\n",
    "\n",
    "# Call the function to find the best LDA configuration\n",
    "bestLDA(Xtr, ytr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best score was obtained with a polynomial degree of 1 and no shrinkage.\n",
    "Degree 1 generally performed better than degree 2, achieving higher accuracy for most shrinkage values, this indicates that a simpler, linear model (degree 1) was more effective for this dataset than a more complex polynomial transformation.\n",
    "In degree 1, shrinkage=None and shrinkage=1e-07 both achieved the highest score,  but with very minimal shrinkage applied.\n",
    "As the shrinkage value increased (e.g., 0.2, 0.3, etc.), the accuracy generally decreased. This suggests that regularization wasn’t beneficial for this particular data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n",
    "\n",
    "# BEST QDA\n",
    "def bestQDA(Xs, ys):\n",
    "    # Define a list of regularization parameter values\n",
    "    reg_param_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.9]\n",
    "    \n",
    "    # Initialize variables to store the best score and corresponding parameters\n",
    "    best_score = 0.0\n",
    "    best_reg_param = 0\n",
    "    best_degree_QDA = 0\n",
    "    \n",
    "    # Print a header to indicate the start of QDA configuration evaluation\n",
    "    print(\"\\nBest QDA Configuration:\")\n",
    "    \n",
    "    # Iterate over polynomial degrees (1 and 2)\n",
    "    for i in range(1, 3):\n",
    "        print(f\"Degree {i}:\")\n",
    "        \n",
    "        # Iterate over the list of regularization parameter values\n",
    "        for reg_param in reg_param_values:\n",
    "            # Use a pipeline with scaling, polynomial expansion, and QDA\n",
    "            pipeline = make_pipeline(\n",
    "                StandardScaler(),\n",
    "                PolynomialFeatures(degree=i),\n",
    "                QuadraticDiscriminantAnalysis(reg_param=reg_param, tol=0)\n",
    "            )\n",
    "            \n",
    "            # Perform 5-fold cross-validation to compute the mean accuracy score\n",
    "            scores = cross_val_score(pipeline, Xs, ys, cv=5, scoring='accuracy')\n",
    "            mean_score = scores.mean()\n",
    "\n",
    "            # Check if the current combination has a higher score and update the best parameters\n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_degree_QDA = i\n",
    "                best_reg_param = reg_param\n",
    "\n",
    "            print(f\"  Reg_param: {reg_param}, Score: {mean_score:.4f}\")\n",
    "        \n",
    "        print()\n",
    "\n",
    "    # Print the best QDA configuration and its results\n",
    "    print(\"Best QDA Configuration:\")\n",
    "    print(f\"Best Degree: {best_degree_QDA}\")\n",
    "    print(f\"Best Reg_param: {best_reg_param}\")\n",
    "    print(f\"Best Score: {best_score:.4f}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Return the best degree and regularization parameter\n",
    "    return best_degree_QDA, best_reg_param\n",
    "\n",
    "# Call the function with input data Xtr and ytr\n",
    "bestQDA(Xtr, ytr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best score was obtained with a polynomial degree of 1 and Reg_param = 0.1.\n",
    "Degree 1 generally performed better than degree 2, achieving higher accuracy, this indicates that a simpler, linear model (degree 1) was more effective for this dataset than a more complex polynomial transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# BEST SVC\n",
    "def bestSVC(Xs, ys):\n",
    "    # Define a range of hyperparameter values for C\n",
    "    C_values = [0.01, 0.1, 1, 10, 100]\n",
    "    # Initialize variables to store the best configuration\n",
    "    best_score = 0.0\n",
    "    best_C = 0\n",
    "    best_degree_SVC = 0\n",
    "    print(\"\\nBest SVC Configuration:\")\n",
    "    \n",
    "    # Iterate over polynomial degrees (1 and 2)\n",
    "    for i in range(1, 3):\n",
    "        print(f\"Evaluating Degree {i}:\")\n",
    "        # Iterate over different values of the hyperparameter C\n",
    "        for C in C_values:\n",
    "            # Apply polynomial feature mapping to the input data\n",
    "            mapper = PolynomialFeatures(degree=i)\n",
    "            Xs_mapped = mapper.fit_transform(Xs)\n",
    "            \n",
    "            # Create and train an SVM classifier (SVC) with the current C value\n",
    "            svc = SVC(C=C)\n",
    "            svc.fit(Xs_mapped, ys)\n",
    "            \n",
    "            # Perform cross-validation to evaluate the model's accuracy\n",
    "            scores = cross_val_score(svc, Xs_mapped, ys, cv=5, scoring='accuracy')\n",
    "            mean_score = scores.mean()\n",
    "            \n",
    "            # Check if the current configuration has a higher score\n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_degree_SVC = i\n",
    "                best_C = C\n",
    "            \n",
    "            print(f\"  C: {C}, Score: {mean_score:.4f}\")\n",
    "\n",
    "    # Print the best SVC configuration and its corresponding parameters\n",
    "    print(\"\\nBest SVC Configuration:\")\n",
    "    print(f\"Best Degree: {best_degree_SVC}\")\n",
    "    print(f\"Best C: {best_C}\")\n",
    "    print(f\"Best Score: {best_score:.4f}\")\n",
    "    print()\n",
    "    \n",
    "    return best_degree_SVC, best_C\n",
    "\n",
    "# Call the bestSVC function with the input data Xtr and ytr\n",
    "bestSVC(Xtr, ytr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best score was obtained with a polynomial degree of 1 and C=10.\n",
    "Degree 1 generally performed better than degree 2, achieving higher accuracy, this indicates that a simpler, linear model (degree 1) was more effective for this dataset than a more complex polynomial transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rAv5h5pcX0IP"
   },
   "source": [
    "---\n",
    "## Part 3: A Harder Dataset\n",
    "\n",
    "OK, now let's switch to the **digits** dataset which should be a bit more challenging. This should be easy by now, and if you have been careful about **functional abstraction** you should be able to reuse much of the functionality from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KBElNvo_X0IQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "ds_digits = load_digits()\n",
    "df_digits = pd.DataFrame(ds_digits.data)     \n",
    "targets_digits = pd.Series(ds_digits.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1: Exploratory data analysis\n",
    "\n",
    "You know the drill, see what this dataset is made of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_digits['DESCR'])\n",
    "print(df_digits.describe())\n",
    "\n",
    "# Access and print the column names (feature names) of the dataset using 'feature_names' attribute of 'ds_digits'.\n",
    "column_names = ds_digits['feature_names']\n",
    "print(column_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. _digits_dataset:\n",
    "\n",
    "Optical recognition of handwritten digits dataset\n",
    "--------------------------------------------------\n",
    "\n",
    "**Data Set Characteristics:**\n",
    "\n",
    "    :Number of Instances: 1797\n",
    "    :Number of Attributes: 64\n",
    "    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n",
    "    :Missing Attribute Values: None\n",
    "    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n",
    "    :Date: July; 1998\n",
    "\n",
    "This is a copy of the test set of the UCI ML hand-written digits datasets\n",
    "https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
    "\n",
    "The data set contains images of hand-written digits: 10 classes where\n",
    "each class refers to a digit.\n",
    "\n",
    "Preprocessing programs made available by NIST were used to extract\n",
    "normalized bitmaps of handwritten digits from a preprinted form. From a\n",
    "total of 43 people, 30 contributed to the training set and different 13\n",
    "to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n",
    "4x4 and the number of on pixels are counted in each block. This generates\n",
    "an input matrix of 8x8 where each element is an integer in the range\n",
    "0..16. This reduces dimensionality and gives invariance to small\n",
    "distortions.\n",
    "\n",
    "For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\n",
    "T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\n",
    "L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n",
    "1994.\n",
    "\n",
    ".. topic:: References\n",
    "\n",
    "  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n",
    "    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n",
    "    Graduate Studies in Science and Engineering, Bogazici University.\n",
    "  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n",
    "  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n",
    "    Linear dimensionalityreduction using relevance weighted LDA. School of\n",
    "    Electrical and Electronic Engineering Nanyang Technological University.\n",
    "    2005.\n",
    "  - Claudio Gentile. A New Approximate Maximal Margin Classification\n",
    "    Algorithm. NIPS. 2000.\n",
    "\n",
    "           0            1            2            3            4   \\\n",
    "count  1797.0  1797.000000  1797.000000  1797.000000  1797.000000   \n",
    "mean      0.0     0.303840     5.204786    11.835838    11.848080   \n",
    "std       0.0     0.907192     4.754826     4.248842     4.287388   \n",
    "min       0.0     0.000000     0.000000     0.000000     0.000000   \n",
    "25%       0.0     0.000000     1.000000    10.000000    10.000000   \n",
    "50%       0.0     0.000000     4.000000    13.000000    13.000000   \n",
    "75%       0.0     0.000000     9.000000    15.000000    15.000000   \n",
    "max       0.0     8.000000    16.000000    16.000000    16.000000   \n",
    "\n",
    "                5            6            7            8            9   ...  \\\n",
    "count  1797.000000  1797.000000  1797.000000  1797.000000  1797.000000  ...   \n",
    "mean      5.781859     1.362270     0.129661     0.005565     1.993879  ...   \n",
    "std       5.666418     3.325775     1.037383     0.094222     3.196160  ...   \n",
    "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
    "25%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
    "50%       4.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
    "75%      11.000000     0.000000     0.000000     0.000000     3.000000  ...   \n",
    "max      16.000000    16.000000    15.000000     2.000000    16.000000  ...   \n",
    "\n",
    "                54           55           56           57           58  \\\n",
    "count  1797.000000  1797.000000  1797.000000  1797.000000  1797.000000   \n",
    "mean      3.725097     0.206455     0.000556     0.279354     5.557596   \n",
    "std       4.919406     0.984401     0.023590     0.934302     5.103019   \n",
    "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
    "25%       0.000000     0.000000     0.000000     0.000000     1.000000   \n",
    "50%       1.000000     0.000000     0.000000     0.000000     4.000000   \n",
    "75%       7.000000     0.000000     0.000000     0.000000    10.000000   \n",
    "max      16.000000    13.000000     1.000000     9.000000    16.000000   \n",
    "\n",
    "                59           60           61           62           63  \n",
    "count  1797.000000  1797.000000  1797.000000  1797.000000  1797.000000  \n",
    "mean     12.089037    11.809126     6.764051     2.067891     0.364496  \n",
    "std       4.374694     4.933947     5.900623     4.090548     1.860122  \n",
    "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
    "25%      11.000000    10.000000     0.000000     0.000000     0.000000  \n",
    "50%      13.000000    14.000000     6.000000     0.000000     0.000000  \n",
    "75%      16.000000    16.000000    12.000000     2.000000     0.000000  \n",
    "max      16.000000    16.000000    16.000000    16.000000    16.000000  \n",
    "\n",
    "[8 rows x 64 columns]\n",
    "['pixel_0_0', 'pixel_0_1', 'pixel_0_2', 'pixel_0_3', 'pixel_0_4', 'pixel_0_5', 'pixel_0_6', 'pixel_0_7', 'pixel_1_0', 'pixel_1_1', 'pixel_1_2', 'pixel_1_3', 'pixel_1_4', 'pixel_1_5', 'pixel_1_6', 'pixel_1_7', 'pixel_2_0', 'pixel_2_1', 'pixel_2_2', 'pixel_2_3', 'pixel_2_4', 'pixel_2_5', 'pixel_2_6', 'pixel_2_7', 'pixel_3_0', 'pixel_3_1', 'pixel_3_2', 'pixel_3_3', 'pixel_3_4', 'pixel_3_5', 'pixel_3_6', 'pixel_3_7', 'pixel_4_0', 'pixel_4_1', 'pixel_4_2', 'pixel_4_3', 'pixel_4_4', 'pixel_4_5', 'pixel_4_6', 'pixel_4_7', 'pixel_5_0', 'pixel_5_1', 'pixel_5_2', 'pixel_5_3', 'pixel_5_4', 'pixel_5_5', 'pixel_5_6', 'pixel_5_7', 'pixel_6_0', 'pixel_6_1', 'pixel_6_2', 'pixel_6_3', 'pixel_6_4', 'pixel_6_5', 'pixel_6_6', 'pixel_6_7', 'pixel_7_0', 'pixel_7_1', 'pixel_7_2', 'pixel_7_3', 'pixel_7_4', 'pixel_7_5', 'pixel_7_6', 'pixel_7_7']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3xOHO3-7UdJ"
   },
   "source": [
    "### Exercise 3.2: Visualize Some Images\n",
    "\n",
    "Now we are working with (very small) images. There a useful Matplotlib function for visualizing images is `imshow()`. Use it like this:\n",
    "\n",
    " `plt.imshow(df_digits.iloc[0,:].to_numpy().reshape(8,8), cmap='gray')`\n",
    "\n",
    " **NOTE**: The Pandas DataFrame structure is kind of getting in our way here -- we have to extract a row, then **convert** it to a numpy array, and then **resize** it to (8,8) to view it.\n",
    "\n",
    "View some images from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kYWRx1ExFe5m"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n",
    "\n",
    "# Plot the 10 classes and their corresponding labels in the dataset\n",
    "for i in range(10):\n",
    "    row = i // 5  # Determine the row position (0 to 4)\n",
    "    col = i % 5   # Determine the column position (0 to 4)\n",
    "\n",
    "    # Display the image using grayscale colormap\n",
    "    axes[row, col].imshow(ds_digits.images[i], cmap='gray')\n",
    "\n",
    "    # Set the title for the subplot with the image label\n",
    "    axes[row, col].set_title(f\"Class: {ds_digits.target[i]}\")\n",
    "\n",
    "# Adjust the spacing between subplots for better layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the subplots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.3: Find the best classifier (of the ones we have seen)\n",
    "\n",
    "You should now be very familiar with this game and if you consolidated the pieces you used before, this exercise should be easy. As always, document, summarize, and justify your conclusions and analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here.\n",
    "Xtr, Xte, ytr, yte = train_test_split(df_digits, targets_digits, test_size=0.75)\n",
    "\n",
    "def bestClassifier(Xtr, ytr):\n",
    "\t# Find the best LDA configuration\n",
    "\tbest_degree_LDA, best_shrinkage = bestLDA(Xtr, ytr)\n",
    "\t\n",
    "\t# Find the best QDA configuration\n",
    "\tbest_degree_QDA, best_reg_param = bestQDA(Xtr, ytr)\n",
    "\t\n",
    "\t# Find the best SVC configuration\n",
    "\tbest_degree_SVC, best_C = bestSVC(Xtr, ytr)\n",
    "\t\n",
    "\tprint(\"Best LDA Configuration:\")\n",
    "\tprint(f\"Degree: {best_degree_LDA}, Shrinkage: {best_shrinkage}\")\n",
    "\t\n",
    "\tprint(\"Best QDA Configuration:\")\n",
    "\tprint(f\"Degree: {best_degree_QDA}, Reg_param: {best_reg_param}\")\n",
    "\t\n",
    "\tprint(\"Best SVC Configuration:\")\n",
    "\tprint(f\"Degree: {best_degree_SVC}, C: {best_C}\")\n",
    "\n",
    "bestClassifier(Xtr, ytr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LDA Analysis`\n",
    "\n",
    "For LDA, I explored two degrees of complexity (Degree 1 and Degree 2) with different regularization parameters (Shrinkage). \n",
    "With Degree 1, the best score was achieved with Shrinkage = 0.2, reaching a score of 0.9399.  However, moving to Degree 2, the Shrinkage = auto option led to a significant improvement, achieving the score of 0.9622, making it the optimal configuration for LDA.\n",
    "\n",
    "The most performant LDA model uses Degree 2 with Shrinkage = auto, reaching a score of 0.9622.\n",
    "\n",
    "`QDA Analysis`\n",
    "\n",
    "For QDA, I tested various values of Reg_param (regularization parameter) for both degrees of complexity.\n",
    "Degree 1 produced the best results with Reg_param = 0.1, achieving a score of 0.9489. In contrast, with Degree 2, the scores dropped drastically, with the highest reaching only 0.0623. This suggests that a second-degree QDA model is too complex for our data, leading to overfitting or instability.\n",
    "\n",
    "The optimal configuration is Degree 1 with Reg_param = 0.1, yielding a score of 0.9489.\n",
    "\n",
    "`SVC Analysis`\n",
    "\n",
    "I tested the SVC model with different regularization values (C) and degrees of complexity.\n",
    "For Degree 1, the best results were obtained with C = 10 and C = 100, both achieving a score of 0.9644. Even with Degree 2, the best scores remained the same (0.9644 with C = 10 or C = 100). However, since the Degree 1 model is simpler and equally performant, it is preferable to choose this configuration.\n",
    "\n",
    "The optimal SVC model uses Degree 1 with C = 10, achieving a score of 0.9644.\n",
    "\n",
    "`Conclusion`\n",
    "\n",
    "The Support Vector Classifier (SVC) with Degree 1 and C = 10 stands out as the best classifier for our data, with a maximum score of 0.9644. However, the LDA with Degree 2 and Shrinkage = auto is also a great alternative, with nearly identical performance (0.9622). This means we can choose SVC if we prefer a more robust model with tighter decision margins, or LDA for a linear approach that is still highly effective."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "FML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
